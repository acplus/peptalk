{
 "metadata": {
  "name": "clustering-new"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Clustering SVM results"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to get meaningful results from our classifier, we cluster positively-predicted residues geometrically.\n",
      "For each receptor entry in PeptiDB, a set of residue clusters is formed, ranked by size.\n",
      "\n",
      "Later we evaluate the success of this method by the recall of binding energy assigned to each cluster separately, and the top-3 combined."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, import all necessary modules. Specifically, note the import of `peptalk` (last line), the module that implements clustering for a given receptor, and some statistics about its success."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import datasets, preprocessing, svm, cross_validation\n",
      "\n",
      "import pylab as pl\n",
      "import pandas as pd\n",
      "\n",
      "import numpy as np\n",
      "import os, sys\n",
      "import csv\n",
      "from pickle import dump, load\n",
      "\n",
      "import peptalk"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The next two cells prepare the SVM output for clustering. \n",
      "The result is `classifier`, an SVM classifier trained on the whole bound receptor set in PeptiDB.\n",
      "\n",
      "We load a *cached version* (\"pickled\") of the `classifier` object, if one is available, \n",
      "to avoid fitting the classifier every time we need its results.\n",
      "If one isn't the classifier is fitted and pickled for subsequent use."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bound_data = pd.read_csv('bound.data.csv', index_col=[0,1])\n",
      "dfeat = bound_data.ix[:,:-1]\n",
      "X_train = preprocessing.scale(dfeat)\n",
      "\n",
      "ddgs = bound_data.ix[:,-1].values\n",
      "y_train = ddgs>0\n",
      "\n",
      "pdbs_train = bound_data.index.get_level_values(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unbound_data = pd.read_csv('unbound.data.csv', index_col=[0,1])\n",
      "X_test = preprocessing.scale(unbound_data.ix[:,:-1])\n",
      "y_test = unbound_data.ix[:,-1].values > 0\n",
      "pdbs_test = unbound_data.index.get_level_values(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "SVM_PICKLE_FILENAME = 'svm.pkl'\n",
      "if os.path.isfile(SVM_PICKLE_FILENAME):\n",
      "    with open(SVM_PICKLE_FILENAME) as clf_file:\n",
      "        classifier = load(clf_file)\n",
      "else:\n",
      "    classifier = svm.SVC(kernel='linear', probability=True, class_weight='auto').fit(X_train,y_train)\n",
      "    with open(SVM_PICKLE_FILENAME, 'w') as clf_file:\n",
      "        dump(classifier, clf_file)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### The DBSCAN algorithm\n",
      "\n",
      "The DBSCAN algorithm views clusters as areas of high density separated by areas of low density. \n",
      "Due to this rather generic view, clusters found by DBSCAN can be any shape, as opposed to k-means \n",
      "which assumes that clusters are convex shaped. \n",
      "\n",
      "The central component to the DBSCAN is the concept of core samples, which are samples that are \n",
      "in areas of high density. A cluster is therefore a set of core samples, each highly similar to \n",
      "each other and a set of non-core samples that are similar to a core sample (but are not \n",
      "themselves core samples). \n",
      "There are two parameters to the algorithm, `min_points` and `eps`, which define formally what we \n",
      "mean when we say dense. \n",
      "A higher `min_points` or lower `eps` indicate higher density necessary to form a cluster.\n",
      "\n",
      "More formally, we define a core sample as being a sample in the dataset such that there exists `min_samples` \n",
      "other samples with a similarity higher than `eps` to it, which are defined as neighbors of the core sample. \n",
      "This tells us that the core sample is in a dense area of the vector space. \n",
      "A cluster is a set of core samples, that can be built by recursively by taking a core \n",
      "sample, finding all of its neighbors that are core samples, finding all of their neighbors \n",
      "that are core samples, and so on. \n",
      "A cluster also has a set of non-core samples, which are samples that are neighbors \n",
      "of a core sample in the cluster but are not themselves core samples. \n",
      "Intuitively, these samples are on the fringes of a cluster.\n",
      "\n",
      "Any core sample is part of a cluster, by definition. Further, any cluster has at least min_samples points in it, following the definition of a core sample. For any sample that is not a core sample, and does not have a similarity higher than eps to a core sample, it is considered an outlier by the algorithm.\n",
      "The algorithm is non-deterministic, however the core samples themselves will always belong to the same clusters (although the labels themselves may be different). The non-determinism comes from deciding on which cluster a non-core sample belongs to. A non-core sample can be have a similarity higher than eps to two core samples in different classes. Following from the triangular inequality, those two core samples would be less similar than eps from each other \u2013 else they would be in the same class. The non-core sample is simply assigned to which ever cluster is generated first, where the order is determined randomly within the code. Other than the ordering of, the dataset, the algorithm is deterministic, making the results relatively stable between iterations on the same data.\n",
      "\n",
      "References:\n",
      "\u201cA Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise\u201d Ester, M., H. P. Kriegel, J. Sander, and X. Xu, In Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, Portland, OR, AAAI Press, pp. 226\u2013231. 1996"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Cluster SVM results for all receptors in PeptiDB 1, and collect statistics in a CSV table about every cluster for subsequent analysis:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pdbid = '1CZY'\n",
      "probs = classifier.predict_proba(X[pdbs==pdbid])[:,1]\n",
      "preds = classifier.predict(X[pdbs==pdbid])\n",
      "#display( zip(preds, probs))\n",
      "r = peptalk.PeptalkResult(pdbid=pdbid, preds=preds, confidence=probs)\n",
      "print r.precision_score(r.cluster_dbscan().values())\n",
      "\n",
      "!cat BindingResidues_alaScan/\"$pdbid\".res"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reload(peptalk)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cluster_stats_fd = open('cluster-stats.csv', 'w')\n",
      "\n",
      "csvwr = csv.writer(cluster_stats_fd,)# delimiter='\\t')\n",
      "\n",
      "csvwr.writerow(\n",
      "                (\n",
      "                'PDBID', \n",
      "                'METHOD', \n",
      "                'RANK', \n",
      "                'SIZE', \n",
      "                'CLUSTER_DDG',\n",
      "                'RECOVERED_DDG',\n",
      "                'TOTAL_DDG',\n",
      "                'ENERGY_RECALL', \n",
      "                'ENERGY_RECALL_POSSIBLE', \n",
      "                'HOTSPOT_RECALL', \n",
      "                'HOTSPOT_PRECISION', \n",
      "                'ENERGY_F2', \n",
      "                'BINDERS_RECALL', \n",
      "                'BINDERS_PRECISION', \n",
      "                'RESIDUES',\n",
      "                )\n",
      "              )\n",
      "\n",
      "for pdbid in set(pdbs_test):\n",
      "    predictions=classifier.predict(X_test[pdbs_test==pdbid])\n",
      "    #print pdbid, predictions\n",
      "    svm_result = peptalk.PeptalkResult(pdbid=pdbid, preds=predictions)\n",
      "    \n",
      "    ward_cl = svm_result.cluster_ward()\n",
      "    dbscan_cl = svm_result.cluster_dbscan()\n",
      "    for method, cluster_dict in (('DBSCAN', dbscan_cl), ('Ward', ward_cl),):\n",
      "        for rank, resnums in cluster_dict.items():\n",
      "            cluster_size = len(resnums)\n",
      "            cluster_ddg = svm_result.cluster_ddg(resnums)\n",
      "            ddg_rcl, ddg_rcl_possible = svm_result.cluster_ddg_recall(resnums)\n",
      "            hs_recl, hs_prec = svm_result.cluster_recall_precision(resnums, svm_result.ddg_resnums)\n",
      "            #bs_recl, bs_prec = svm_result.cluster_recall_precision(resnums, svm_result.binders_resnums)\n",
      "            def fbeta_score(recall, precision, beta=1):\n",
      "                if recall==0 and precision==0: return 0\n",
      "                return (1 + beta**2) * float(precision*recall) / float(beta**2 * precision + recall)\n",
      "            f2_ddg = fbeta_score(recall=ddg_rcl_possible, precision=hs_prec, beta=1)\n",
      "            csvwr.writerow([\n",
      "                            pdbid,\n",
      "                            method,\n",
      "                            rank,\n",
      "                            cluster_size,\n",
      "                            cluster_ddg,\n",
      "                            svm_result.recovered_ddg,\n",
      "                            svm_result.total_ddg,\n",
      "                            ddg_rcl,\n",
      "                            ddg_rcl_possible,\n",
      "                            hs_recl,\n",
      "                            hs_prec,\n",
      "                            f2_ddg,\n",
      "                            #bs_recl,\n",
      "                            #bs_prec,\n",
      "                            ' '.join(map(str, resnums)),\n",
      "                            ])\n",
      "        #break\n",
      "\n",
      "cluster_stats_fd.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Read the CSV table generated above into `cluster_stats`, \n",
      "a Data Frame object - kind of a spreadsheet that can hold data and manipulate it in cool ways."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cluster_stats = pd.read_csv('cluster-stats.csv',)\n",
      "# here's a quick peek at the top 5 rows:\n",
      "display(cluster_stats.ix[cluster_stats.PDBID=='1AWR'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create an object called `colnames` to represent column names programmatically.\n",
      "It saves the need to specify a magic string when addressing a column name in the data frame.\n",
      "\n",
      "So instead of `cluster_stats['PDBID']` you can write `cluster_stats[colnames.PDBID]`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Struct:\n",
      "    def __init__(self, **entries): \n",
      "        self.__dict__.update(entries)\n",
      "\n",
      "cols = cluster_stats.columns.tolist()\n",
      "colnames = Struct(**dict(zip(cols, cols)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Cluster size distribution in DBSCAN results"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "db_clusters = cluster_stats.ix[\n",
      "                    #(cluster_stats.METHOD=='DBSCAN') & \n",
      "                    (cluster_stats.ENERGY_RECALL_POSSIBLE > 0),\n",
      "                    (colnames.METHOD,colnames.SIZE)\n",
      "                    ]\n",
      "size_pvt = db_clusters.pivot_table(\n",
      "            rows=colnames.SIZE, \n",
      "            #cols=colnames.RANK, \n",
      "            cols=colnames.METHOD,\n",
      "            aggfunc=np.count_nonzero, \n",
      "            fill_value=0,\n",
      "        )\n",
      "size_pvt.plot()#subplots=True, sharex=True, sharey=True, yticks=range(0,61,20), )\n",
      "pl.ylabel('Frequency')\n",
      "pl.xlabel('Cluster size')\n",
      "pl.show()\n",
      "#savefig('interactive1.svg')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Performance comparison between DBSCAN and Ward clustering"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Define `topk_performance`, a function that gathers data about the top-k clusters of each method in a pivot table,\n",
      "outputs a PDBID-wise comparison between them, generates a bar chart of their recall distribution and summarizing statistics."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rcParams['text.usetex'] = True\n",
      "\n",
      "def topk_performance(cluster_stats, k=1):\n",
      "    # filter the data to ranks < k, and where the SVM classified positively \n",
      "    # enough residues to comprise at least 50% of the total ddG\n",
      "    topk_clusters = cluster_stats.ix[\n",
      "                        (cluster_stats.RANK<k) \n",
      "                        & (cluster_stats.RECOVERED_DDG > .5 * cluster_stats.TOTAL_DDG) \n",
      "                        #& (cluster_stats.METHOD=='DBSCAN')\n",
      "                        ]\n",
      "    \n",
      "    # pivot the data to aggregate the top-k clusters for each method\n",
      "    # and report the sum of ddG-related parameters over the top-k clusters\n",
      "    method_pvt = topk_clusters.pivot_table(\n",
      "                rows=colnames.PDBID, \n",
      "                cols=  [\n",
      "                        colnames.METHOD,\n",
      "                        #colnames.RANK,\n",
      "                        ], \n",
      " \n",
      "                values=[\n",
      "                        colnames.ENERGY_RECALL,\n",
      "                        colnames.ENERGY_RECALL_POSSIBLE,\n",
      "                        colnames.ENERGY_F2,\n",
      "                        #colnames.RECOVERED_DDG,\n",
      "                        #colnames.TOTAL_DDG,\n",
      "                        colnames.HOTSPOT_RECALL,\n",
      "                        colnames.BINDERS_RECALL, \n",
      "                        #colnames.HOTSPOT_PRECISION, \n",
      "                        #colnames.BINDERS_PRECISION,\n",
      "                       ], \n",
      "                        aggfunc=np.sum,\n",
      "                        #margins=True,\n",
      "              )\n",
      "    recalls=method_pvt[colnames.ENERGY_RECALL_POSSIBLE]\n",
      "    \n",
      "    display(method_pvt.head(10))#.ix[:,:4])\n",
      "    \n",
      "    pl.hist([recalls[col] for col in recalls.columns], \n",
      "            bins=linspace(0, 1, num=6, endpoint=True), \n",
      "            label=recalls.columns.tolist(), \n",
      "            alpha=.8,\n",
      "            )\n",
      "    \n",
      "    pl.title('''Distribution of $\\Delta\\Delta G$ recall of top-%d clusters \n",
      "                for DBSCAN and Ward clustering methods''' % k)\n",
      "    pl.grid(axis='y')\n",
      "    pl.legend(loc='upper left')\n",
      "    pl.xlabel('Success rate ($ \\Delta\\Delta G $ recall)')\n",
      "    pl.ylabel('Frequency')\n",
      "    \n",
      "    pl.show()\n",
      "    \n",
      "    # overall performance of the two methods over different parameters\n",
      "    m=method_pvt.mean().unstack(1)\n",
      "    display(m)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Performance of top cluster in each method"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "topk_performance(cluster_stats, k=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Performance of top-3 clusters of each method"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "topk_performance(cluster_stats, k=3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Parameter comparison on DBSCAN results"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this part I compare the success rate I measure over two definitions of the binding site. \n",
      "This leads me to lean toward the positive-ddG energy-based definition, rather than the 4A-cutoff distance-based definition."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "The ddG recovered by different classifiers"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ddg_pvt = cluster_stats.ix[\n",
      "                    (cluster_stats.RANK==0) &\n",
      "                    (cluster_stats.METHOD=='DBSCAN')\n",
      "                    ].pivot_table(\n",
      "                rows=colnames.PDBID, \n",
      "                cols=  [\n",
      "                        #colnames.METHOD,\n",
      "                        #colnames.RANK,\n",
      "                        ], \n",
      " \n",
      "                values=[\n",
      "                        #colnames.ENERGY_RECALL,\n",
      "                        #colnames.ENERGY_RECALL_POSSIBLE,\n",
      "                        #colnames.ENERGY_F2,\n",
      "                        #colnames.CLUSTER_DDG,\n",
      "                        colnames.RECOVERED_DDG,\n",
      "                        colnames.TOTAL_DDG,\n",
      "                        #colnames.HOTSPOT_RECALL,\n",
      "                        #colnames.BINDERS_RECALL, \n",
      "                        #colnames.HOTSPOT_PRECISION, \n",
      "                        #colnames.BINDERS_PRECISION,\n",
      "                       ], \n",
      "                        aggfunc=np.sum,\n",
      "                        #margins=True,\n",
      "              )\n",
      "ddg_pvt['RECOVERY_RATE'] = ddg_pvt.RECOVERED_DDG / ddg_pvt.TOTAL_DDG\n",
      "display(ddg_pvt.head())\n",
      "display(ddg_pvt.describe())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Obsolete content"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "coloring clusters by rank"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Visualizing clustering results with ProDy `showProtein`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "prody.SETTINGS['auto_show']=False\n",
      "showProtein(prot, A='grey', B='magenta', label='prot', lw=3)\n",
      "s = positives.copy()\n",
      "s.setTitle('positive')\n",
      "cluster_colors = {\n",
      "        'CL0_': 'Red',\n",
      "        'CL1_': 'DarkOrange',\n",
      "        'CL2_': 'Coral',\n",
      "        'CL3_': 'Yellow',\n",
      "        'CL4_': 'White',\n",
      "        'CL-1_': 'Green',\n",
      "        }\n",
      "   \n",
      "s.ca.setResnames(['CL%s_' % str(label) for label in cluster_labels])\n",
      "print cluster_colors\n",
      "showProtein(s, hsize=10, label='positives', **cluster_colors)\n",
      "pl.legend(loc='upper left')\n",
      "pl.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Produce a copy of `prot` with residues in `resnums` having beta numbers from `bfactors`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def modifiedBetas(prot, resnums, bfactors):\n",
      "    p = prot.copy()\n",
      "    p.setBetas([0]*len(p))\n",
      "    \n",
      "    h = p.select('chain A and resnum %s' % ' '.join(map(str, resnums))).getHierView()\n",
      "    for resnum, bfactor in zip(resnums, bfactors):\n",
      "        res = h.getResidue('A', resnum)\n",
      "        res.setBetas([bfactor]*len(res))\n",
      "    \n",
      "    return p"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "scoring clusters using size and planarity"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Measure the planarity of a set of coordinates using PCA:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def planarity(coords):\n",
      "    pca = decomposition.PCA()\n",
      "    pca.fit(coords)\n",
      "    return 1-pca.explained_variance_ratio_[-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Score a set of clusters based on size and planarity:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def score_clusters(coords, cluster_labels):\n",
      "    clusters = [coords[cluster_labels==i] for i in range(N_CLUSTERS)]\n",
      "    planarities = np.array([planarity(cl_coords) for cl_coords in clusters])\n",
      "    plana = planarities #preprocessing.scale(planarities)\n",
      "    \n",
      "    print plana\n",
      "    sizes = np.array(map(len, clusters))\n",
      "    print sizes\n",
      "    \n",
      "    scores = plana * sizes\n",
      "    return scores\n",
      "    #print len(coords), planarity,\n",
      "    #return len(coords) * planarity"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}